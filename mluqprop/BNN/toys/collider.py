#!/usr/bin/env python

"""
1D Epistemic Uncertainty - Model Form
================================

This script demonstrates the performance of an epistemic Bayesian net
on a 1-dimensional dataset generated by a colliding causal model:
                        X --> Z <-- Y
where X can be modeled, but Y cannot. This is meant to simulate the
turbulence closure model where coarse-graining uncertainty yields an
aleatoric form of uncertainty.

Moreover, there are two distinct regions of data:
(a) high epistemic (sparse data), low aleatoric (low noise) uncertainty
(b) low epistemic (dense data), high aleatoric (large noise) uncertainty
"""

import argparse
import os
import sys
from time import time

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from prettyPlot.plotting import pretty_labels, pretty_legend

from mluqprop.BNN.util import compute_predictions, compute_prob_predictions
from mluqprop.BNN.util.input_parser import check_user_input, parse_input_deck
from mluqprop.BNN.util.models import BNNHyperModel, mlp, neg_loglik

__author__ = "Graham Pash"


def main(args):
    #############################
    # Parse the input deck.
    #############################
    if args.input is None:
        raise NameError(
            "No input deck provided. Please specify with: --input <input_deck>"
        )
    simparams = parse_input_deck(args.input)

    np.random.seed(simparams.seed)  # for reproducibility
    SAVEDIR = args.savedir
    OPTIM = simparams.optimizer
    VAROPTIM = OPTIM  # TODO: use SVGD

    #############################
    # Synthetic Data Generation.
    #############################
    xlim_train1 = [-1.5, -0.5]
    xlim_train2 = [0.5, 1.5]
    xlim_test = [-3, 3]
    D_X = 1  # one-dimensional dataset.
    D_Y = 1  # one-dimensional output.

    # Unpack input arguments.
    Ntrain1 = round(simparams.num_train_data / simparams.data_ratio)
    Ntrain2 = 10 * Ntrain1
    Ntrain = Ntrain1 + Ntrain2
    Ntest = simparams.num_test_data
    D_H = simparams.hidden_dim
    percentiles = [
        50.0 - simparams.percentiles / 2.0,
        50.0 + simparams.percentiles / 2.0,
    ]

    # Generate data.
    ydata = lambda x, n: np.power(x, 3) + simparams.noise_level * (
        1.5 + x
    ) * np.random.randn(n)

    Xtrain1 = np.linspace(xlim_train1[0], xlim_train1[1], Ntrain1)
    Xtrain2 = np.linspace(xlim_train2[0], xlim_train2[1], Ntrain2)
    Xtrain = np.concatenate([Xtrain1, Xtrain2])
    Ytrain = ydata(Xtrain.squeeze(), Ntrain)[:, np.newaxis]

    Xtest = np.linspace(xlim_test[0], xlim_test[1], Ntest)[:, np.newaxis]
    Ytest = ydata(Xtest.squeeze(), Ntest)[:, np.newaxis]

    # If training a deterministic network, do that first.
    if args.do_mlp:
        print("Training Multilayer Perceptron")

        # Define callbacks.
        checkpoint_filepath = os.path.join(simparams.checkpath, "mlp")
        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_filepath,
            save_weights_only=True,
            monitor="loss",
            mode="min",
            save_best_only=True,
        )

        # Instantiate model, print information, compile.
        mlp_model = mlp(D_X=1, D_H=D_H, activation_fn=simparams.nonlin)
        mlp_model.summary()
        starttime = time()
        mlp_model.compile(
            optimizer=OPTIM,
            loss="mse",
        )

        # Train model.
        mlp_history = mlp_model.fit(
            Xtrain,
            Ytrain,
            epochs=simparams.max_iter,
            verbose=args.verbose,
            callbacks=[model_checkpoint_callback],
        )
        print(f"Elapsed full model training time: {time() - starttime:.2f}s")

        # Restore best model.
        mlp_model.load_weights(checkpoint_filepath).expect_partial()
        mlp_pred = mlp_model(Xtest)

    #############################
    # Epistemic Uncertainty ONLY
    #############################
    print("Training Epistemic BNN.")

    # Define callbacks.
    checkpoint_filepath = os.path.join(simparams.checkpath, "epi")
    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        save_weights_only=True,
        monitor="loss",
        mode="min",
        save_best_only=True,
    )

    # Instatiantiate model, print information, compile.
    abstractmodel = BNNHyperModel(
        dx=D_X,
        dh=simparams.hidden_dim,
        dy=D_Y,
        nh=simparams.num_layers,
        kl_weight=1 / Ntrain,
        model_type="epi",
        activation_fn=simparams.nonlin,
        posterior_model="mvn",
        prior_model="trainable",
    )
    epi_model = abstractmodel.build()
    epi_model.compile(
        optimizer=VAROPTIM,
        loss="mse",
    )
    epi_model.summary()

    # Train model.
    starttime = time()
    epi_history = epi_model.fit(
        Xtrain,
        Ytrain,
        epochs=simparams.max_iter,
        verbose=args.verbose,
        callbacks=[model_checkpoint_callback],
    )
    print(f"Elapsed epistemic training time: {time() - starttime:.2f}s")

    # Load best weights, make predictions.
    epi_model.load_weights(checkpoint_filepath).expect_partial()
    epi_preds, epi_mean, epi_ptiles = compute_predictions(
        epi_model, Xtest, num_samples=simparams.num_samples, ptiles=percentiles
    )

    #############################
    # Epistemic & Aleatoric
    #############################
    print("Training Epistemic & Aleatoric BNN.")

    # Define callbacks.
    checkpoint_filepath = os.path.join(simparams.checkpath, "bnn")
    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_filepath,
        save_weights_only=True,
        monitor="loss",
        mode="min",
        save_best_only=True,
    )

    # Instatiantiate model, print information, compile.
    abstractmodel = BNNHyperModel(
        dx=D_X,
        dh=simparams.hidden_dim,
        dy=D_Y,
        nh=simparams.num_layers,
        kl_weight=1 / Ntrain,
        model_type="variational",
        activation_fn=simparams.nonlin,
        posterior_model="mvn",
        prior_model="trainable",
    )
    bnn_model = abstractmodel.build()

    bnn_model.compile(
        optimizer=VAROPTIM,
        loss=neg_loglik,
    )
    bnn_model.summary()

    # Train model.
    starttime = time()
    bnn_history = bnn_model.fit(
        Xtrain,
        Ytrain,
        epochs=simparams.max_iter,
        verbose=args.verbose,
        callbacks=[model_checkpoint_callback],
    )
    print(f"Elapsed full model training time: {time() - starttime:.2f}s")

    # Load best weights, make predictions.
    bnn_model.load_weights(checkpoint_filepath).expect_partial()
    bnn_preds, bnn_mean, bnn_ptiles = compute_prob_predictions(
        bnn_model, Xtest, num_samples=simparams.num_samples, ptiles=percentiles
    )

    #############################
    # Plotting
    #############################

    # Plot data.
    fig, ax = plt.subplots()
    # plt.scatter(Xtest, Ytest, alpha=0.5, color='gray', label='Test Data')
    plt.plot(Xtrain, Ytrain, "kx", label="Training Data")

    pretty_labels(xlabel="x", ylabel="y", fontsize=15)
    plt.xlim(xlim_test)
    plt.savefig(os.path.join(SAVEDIR, "uncertainty-cartoon.pdf"))
    plt.close()

    # Plot for the epistemic model
    fig, ax = plt.subplots()
    # plt.scatter(Xtest, Ytest, alpha=0.5, color='gray', label='Test Data')
    plt.plot(
        Xtrain, Ytrain, "x", alpha=0.5, color="gray", label="Training Data"
    )

    # Add epistemic model to plot.
    ax.plot(
        Xtest, epi_mean, "blue", ls="solid", lw=2.0, label="Epistemic Only"
    )

    # plot 90% confidence level of predictions
    ax.fill_between(
        Xtest.squeeze(),
        epi_ptiles[0, :],
        epi_ptiles[1, :],
        color="lightblue",
        alpha=0.5,
    )

    if args.do_mlp:
        # Add MLP prediction to plot.
        plt.plot(
            Xtest, mlp_pred, "k", linewidth=2.0, label="Multilayer Perceptron"
        )

    plt.ylim(-5, 5)
    plt.xlim(xlim_test)
    pretty_labels(xlabel="x", ylabel="y", fontsize=18)
    pretty_legend()
    plt.savefig(os.path.join(SAVEDIR, f"epistemic_model.pdf"))
    plt.close()

    # Plot the full Bayesian model.
    fig, ax = plt.subplots()
    # plt.scatter(Xtest, Ytest, alpha=0.5, color='gray', label='Test Data')
    plt.plot(
        Xtrain, Ytrain, "x", alpha=0.5, color="gray", label="Training Data"
    )

    ax.plot(
        Xtest,
        bnn_mean,
        "orangered",
        ls="solid",
        lw=2.0,
        label="Epistemic+Aleatoric",
    )

    # plot 90% confidence level of predictions
    ax.fill_between(
        Xtest.squeeze(),
        bnn_ptiles[0, :],
        bnn_ptiles[1, :],
        color="lightcoral",
        alpha=0.5,
    )

    if args.do_mlp:
        # Add MLP prediction to plot.
        plt.plot(
            Xtest, mlp_pred, "k", linewidth=2.0, label="Multilayer Perceptron"
        )

    pretty_labels(xlabel="x", ylabel="y", fontsize=18)
    pretty_legend()
    plt.ylim(-5, 5)
    plt.xlim(xlim_test)
    plt.savefig(os.path.join(SAVEDIR, f"full_model.pdf"))
    plt.close()

    #############################
    # Plot Individual Realizations
    #############################
    Nplots = simparams.num_plots

    # Epistemic model.
    fig, ax = plt.subplots()
    for _ in range(Nplots):
        pred = epi_model(Xtest).numpy()
        plt.plot(Xtest, pred, "k", lw=0.5)
    # plot 90% confidence level of predictions
    ax.fill_between(
        Xtest.squeeze(),
        epi_ptiles[0, :],
        epi_ptiles[1, :],
        color="gray",
        alpha=0.5,
    )
    plt.xlabel("x")
    plt.ylabel("y")
    plt.title("Epistemic Model Realizations")
    plt.savefig(os.path.join(SAVEDIR, "epi_realizations.pdf"))

    # Epistemic & Aleatoric model.
    fig, ax = plt.subplots()
    for _ in range(Nplots):
        pred = bnn_model(Xtest).sample()
        plt.plot(Xtest, pred, "k", lw=0.5)
    # plot 90% confidence level of predictions
    ax.fill_between(
        Xtest.squeeze(),
        bnn_ptiles[0, :],
        bnn_ptiles[1, :],
        color="gray",
        alpha=0.5,
    )
    plt.xlabel("x")
    plt.ylabel("y")
    plt.title("Full Model Realizations")
    plt.savefig(os.path.join(SAVEDIR, "full_realizations.pdf"))

    # Plot Training histories
    if args.save_history:
        # Epistemic model.
        fig, ax = plt.subplots()
        ax.set_yscale("log")
        plt.plot(epi_history.history["loss"])
        pretty_labels(
            xlabel="Iteration",
            ylabel="ELBO",
            fontsize=18,
            title="Epistemic Training History",
        )
        plt.savefig(os.path.join(SAVEDIR, "epi_history.pdf"))
        plt.close()

        # Full Bayesian model.
        fig, ax = plt.subplots()
        ax.set_yscale("log")
        plt.plot(bnn_history.history["loss"])
        pretty_labels(
            xlabel="Iteration",
            ylabel="ELBO",
            fontsize=18,
            title="Epistemic+Aleatoric Training History",
        )
        plt.savefig(os.path.join(SAVEDIR, "full_history.pdf"))
        plt.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Bayesian Neural Network example on 1D dataset"
    )

    # Set up input arguments.
    parser.add_argument("--input", help="Input deck.")
    parser.add_argument(
        "--savedir",
        default="../Figures/collider/",
        help="Output directory for figures.",
    )
    parser.add_argument(
        "--verbose", action=argparse.BooleanOptionalAction, default=False
    )
    parser.add_argument(
        "--save-history", action=argparse.BooleanOptionalAction, default=True
    )
    parser.add_argument(
        "--do-mlp", action=argparse.BooleanOptionalAction, default=True
    )

    args = parser.parse_args()

    if args.verbose:
        print(args)

    # ######## Run Script ########
    main(args)
